{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "H2FmZxvftfj1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision.io import read_image\n",
        "from torchvision import transforms\n",
        "from torchvision.ops import DeformConv2d\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "SKhi6sEatrF9"
      },
      "outputs": [],
      "source": [
        "class ISARDataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.palette = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "        image = read_image(img_path).float()\n",
        "        # image = transforms.ToTensor()(image).float() # Convert to float\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        image2 = Image.open(img_path)\n",
        "\n",
        "        if self.palette is None:\n",
        "            self.palette = np.array(image2.getpalette()).reshape(-1, 3)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "6V4fHWkQl_Y7"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LowerConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv_blocks = nn.Sequential()  # Use nn.Sequential to hold blocks\n",
        "\n",
        "        in_channels = 1  # Start with 1 input channel (grayscale)\n",
        "        for out_channels, num_repeats in [(8, 4), (16, 3), (32, 3)]:\n",
        "            for _ in range(num_repeats):\n",
        "                self.conv_blocks.append(\n",
        "                    nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=0)\n",
        "                )\n",
        "                self.conv_blocks.append(nn.ReLU())\n",
        "                self.conv_blocks.append(nn.BatchNorm2d(out_channels))\n",
        "                in_channels = out_channels  # Update in_channels for next block\n",
        "\n",
        "            self.conv_blocks.append(nn.MaxPool2d(2, 2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv_blocks(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "VIrQyihkmuyo"
      },
      "outputs": [],
      "source": [
        "class DeformedAffineConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # x = x.view(-1, 1, H, W)\n",
        "        kh, kw = 3, 3\n",
        "\n",
        "        self.offset_generator1 = nn.Conv2d(32, 2 * kh * kw, kernel_size=3)  # Offset generator\n",
        "        self.deform_conv1 = DeformConv2d(32, 64, kernel_size=3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.offset_generator2 = nn.Conv2d(64, 2 * kh * kw, kernel_size=3)  # Offset generator\n",
        "        self.deform_conv2 = DeformConv2d(64, 128, kernel_size=3)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(128, 6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        offset1 = self.offset_generator1(x)\n",
        "        x = self.deform_conv1(x, offset1)\n",
        "        x = self.pool(self.bn1(F.relu(x)))\n",
        "\n",
        "        offset2 = self.offset_generator2(x)\n",
        "        x = self.deform_conv2(x, offset2)\n",
        "        x = self.bn2(F.relu(x))\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "mZrq77esc4sp"
      },
      "outputs": [],
      "source": [
        "class DeformedShrinkConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        kh, kw = 3, 3\n",
        "        self.offset_generator1 = nn.Conv2d(32, 2 * kh * kw, kernel_size=3)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.deform_conv1 = DeformConv2d(32, 64, kernel_size=3)\n",
        "\n",
        "        self.offset_generator2 = nn.Conv2d(64, 2 * kh * kw, kernel_size=3)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.deform_conv2 = DeformConv2d(64, 128, kernel_size=3)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(128, 64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        offset1 = self.offset_generator1(x)\n",
        "        x = self.deform_conv1(x, offset1)\n",
        "        x = self.pool(self.bn1(F.relu(x)))\n",
        "\n",
        "        offset2 = self.offset_generator2(x)\n",
        "        x = self.deform_conv2(x, offset2)\n",
        "        x = self.bn2(F.relu(x))\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "atwrHiFlmfSW"
      },
      "outputs": [],
      "source": [
        "class SAISARNet(nn.Module):\n",
        "    def __init__(self, num_classes, F=3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Lower ConvNet (Shared)\n",
        "        self.lower_convnet = LowerConvNet()\n",
        "\n",
        "        # Deformed Affine ConvNet (Shares parameters with Lower ConvNet)\n",
        "        self.deform_affine = nn.Sequential(\n",
        "            self.lower_convnet,\n",
        "            DeformedAffineConvNet()\n",
        "        )\n",
        "\n",
        "        # Deformed Shrink ConvNet (Independent Parameters)\n",
        "        self.deform_shrink = nn.Sequential(\n",
        "            LowerConvNet(),\n",
        "            DeformedShrinkConvNet()\n",
        "        )\n",
        "\n",
        "        # Bi-LSTM and Attention\n",
        "        self.bilstm = nn.LSTM(128*6*6, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(256, 1),  # Attention mechanism (2 * hidden_size)\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # batch_size, F, _, H, W = x.shape\n",
        "        # x = x.view(-1, 1, H, W)\n",
        "        print('hi')\n",
        "\n",
        "        # Shared Lower ConvNet\n",
        "        features = self.lower_convnet(x)\n",
        "        # features = features.view(batch_size, F, 8, 60, 60)\n",
        "        print('hi')\n",
        "\n",
        "        # Global Adjustment\n",
        "        global_params = self.deform_affine(features[:, 0])\n",
        "        global_params = global_params.view(-1, 2, 3)\n",
        "\n",
        "        print('hi')\n",
        "        # Apply affine transformation to all frames\n",
        "        adjusted_features = []\n",
        "        for f in range(F):\n",
        "            grid = F.affine_grid(global_params, x[:, f].size(), align_corners=False)\n",
        "            transformed_frame = F.grid_sample(x[:, f], grid, align_corners=False)\n",
        "            adjusted_features.append(transformed_frame)\n",
        "        adjusted_features = torch.stack(adjusted_features, dim=1)\n",
        "        print('hi')\n",
        "\n",
        "        # Local Adjustment\n",
        "        shrink_features = self.deform_shrink(features)\n",
        "\n",
        "        # Bi-LSTM and Attention\n",
        "        shrink_features = shrink_features.view(shrink_features.size(0), -1)\n",
        "        lstm_out, _ = self.bilstm(shrink_features)\n",
        "        attention_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
        "        weighted_lstm_out = torch.sum(lstm_out * attention_weights, dim=1)\n",
        "        print('hi')\n",
        "\n",
        "        # Classification\n",
        "        output = self.classifier(weighted_lstm_out)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jWLjefbmVmE"
      },
      "source": [
        "## Temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "5tr8JfVmmXza"
      },
      "outputs": [],
      "source": [
        "# pass\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# from torchvision.ops import DeformConv2d\n",
        "\n",
        "# class SAISARNet(nn.Module):\n",
        "#     # ... (your existing __init__ method remains the same)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         batch_size, F, _, H, W = x.shape  # Get dimensions (assuming input shape: (batch_size, F, 1, H, W))\n",
        "#         x = x.view(-1, 1, H, W)  # Reshape for lower_convnet (batch_size * F, 1, H, W)\n",
        "\n",
        "#         # Shared Lower ConvNet\n",
        "#         features = self.lower_convnet(x)\n",
        "#         features = features.view(batch_size, F, 32, 9, 9)  # Reshape back to (batch_size, F, 32, 9, 9)\n",
        "\n",
        "#         # Global Adjustment\n",
        "#         global_params = self.deform_affine(features[:, 0])\n",
        "#         global_params = global_params.view(-1, 2, 3)\n",
        "\n",
        "#         # Apply affine transformation to all frames\n",
        "#         adjusted_features = []\n",
        "#         for f in range(F):\n",
        "#             grid = F.affine_grid(global_params, x[:, f].size(), align_corners=False)\n",
        "#             transformed_frame = F.grid_sample(x[:, f], grid, align_corners=False)\n",
        "#             adjusted_features.append(transformed_frame)\n",
        "#         adjusted_features = torch.stack(adjusted_features, dim=1)\n",
        "\n",
        "#         # Local Adjustment\n",
        "#         shrink_features = self.deform_shrink(adjusted_features)\n",
        "#         shrink_features = shrink_features.view(batch_size, F, -1)  # Flatten for LSTM\n",
        "\n",
        "#         # Bi-LSTM and Attention\n",
        "#         lstm_out, _ = self.bilstm(shrink_features)\n",
        "\n",
        "#         attention_weights = self.attention(lstm_out)  # Calculate attention weights\n",
        "#         attention_weights = torch.softmax(attention_weights, dim=1)\n",
        "\n",
        "#         # Apply attention and aggregate features\n",
        "#         weighted_features = torch.bmm(lstm_out.transpose(1, 2), attention_weights).squeeze(2)\n",
        "\n",
        "#         # Classification\n",
        "#         output = self.classifier(weighted_features)\n",
        "#         return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "3aRVmGevdwGt"
      },
      "outputs": [],
      "source": [
        "# pass\n",
        "# class SAISARNet(nn.Module):\n",
        "#     def __init__(self, num_classes, F=3):  # F: number of frames in the sequence\n",
        "#         super().__init__()\n",
        "\n",
        "#         # self.conv_layers = nn.Sequential(\n",
        "#         #     # Block 1: Conv.8@3x3, BN/ReLU x4\n",
        "#         #     nn.Conv2d(1, 8, kernel_size=3, padding=0),  # 1 input channel (grayscale)\n",
        "#         #     nn.ReLU(),\n",
        "#         #     nn.BatchNorm2d(8),\n",
        "#         #     nn.Conv2d(8, 8, kernel_size=3, padding=0),\n",
        "#         #     nn.ReLU(),\n",
        "#         #     nn.BatchNorm2d(8),\n",
        "#         #     nn.Conv2d(8, 8, kernel_size=3, padding=0),\n",
        "#         #     nn.ReLU(),\n",
        "#         #     nn.BatchNorm2d(8),\n",
        "#         #     nn.Conv2d(8, 8, kernel_size=3, padding=0),\n",
        "#         #     nn.ReLU(),\n",
        "#         #     nn.BatchNorm2d(8),\n",
        "#         #     nn.MaxPool2d(2, 2))\n",
        "\n",
        "#         #     # Block 2: Conv.16@3x3, BN/ReLU x3\n",
        "#         # self.conv_layer2 = nn.Sequential(\n",
        "#         #     nn.Conv2d(8, 16, kernel_size=3),\n",
        "#         #     nn.ReLU(),\n",
        "#         #     nn.BatchNorm2d(16),\n",
        "#         #     nn.Conv2d(16, 16, kernel_size=3),\n",
        "#         #     nn.ReLU(),\n",
        "#         #     nn.BatchNorm2d(16),\n",
        "#         #     nn.Conv2d(16, 16, kernel_size=3),\n",
        "#         #     nn.ReLU(),\n",
        "#         #     nn.BatchNorm2d(16),\n",
        "#         #     nn.MaxPool2d(2, 2))\n",
        "#         # self.conv_layer3 = nn.Sequential(\n",
        "#         #     # # Block 3: Conv.16@3x3, BN/ReLU x3\n",
        "#         #     nn.Conv2d(16, 32, kernel_size=3),\n",
        "#         #     nn.ReLU(),\n",
        "#         #     nn.BatchNorm2d(32),\n",
        "#         #     nn.Conv2d(32, 32, kernel_size=3),\n",
        "#         #     nn.ReLU(),\n",
        "#         #     nn.BatchNorm2d(32),\n",
        "#         #     nn.Conv2d(32, 32, kernel_size=3),\n",
        "#         #     nn.ReLU(),\n",
        "#         #     nn.BatchNorm2d(32),\n",
        "#         #     nn.MaxPool2d(2, 2))  # Output 32x9x9\n",
        "\n",
        "\n",
        "#         def forward(self, x):\n",
        "#             return self.conv_layer2(self.conv_layers(x))\n",
        "\n",
        "#         # Lower ConvNet (Shared)\n",
        "#         # self.lower_convnet = nn.Sequential(*layers)\n",
        "#         #     nn.Conv2d(1, 8, kernel_size=3, padding=1),  # Input 1 channel (grayscale)\n",
        "#         #     nn.ReLU(),\n",
        "#         #     nn.BatchNorm2d(8),\n",
        "#         #     nn.MaxPool2d(2, 2),\n",
        "\n",
        "#         #     nn.Conv2d(8, 16, kernel_size=3, padding=1),\n",
        "#         #     nn.ReLU(),\n",
        "#         #     nn.BatchNorm2d(16),\n",
        "#         #     nn.MaxPool2d(2, 2),\n",
        "\n",
        "#         #     nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "#         #     nn.ReLU(),\n",
        "#         #     nn.BatchNorm2d(32),\n",
        "#         #     nn.MaxPool2d(2, 2)  # Output 32x9x9\n",
        "#         # )\n",
        "\n",
        "#         # Deformed Affine ConvNet\n",
        "#         self.deform_affine = nn.Sequential(\n",
        "#             DeformConv2d(32, 64, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(64),\n",
        "#             nn.MaxPool2d(2, 2),\n",
        "\n",
        "#             DeformConv2d(64, 128, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Flatten(),\n",
        "#             nn.Linear(128, 6)  # Output affine transformation parameters (2x3)\n",
        "#         )\n",
        "\n",
        "#         # Deformed Shrink ConvNet\n",
        "#         self.deform_shrink = nn.Sequential(\n",
        "#             DeformConv2d(32, 64, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(2, 2),\n",
        "#             DeformConv2d(64, 128, kernel_size=3, padding=1),\n",
        "#             nn.ReLU()\n",
        "#         )\n",
        "\n",
        "#         # Bi-LSTM and Attention\n",
        "#         self.bilstm = nn.LSTM(128, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
        "#         self.attention = nn.Sequential(\n",
        "#             nn.Linear(256, 1),  # Attention mechanism (2 * hidden_size)\n",
        "#             nn.Tanh()\n",
        "#         )\n",
        "#         self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "#     # def forward(self, x):\n",
        "#     #     # Assuming input x is (batch_size, F, 120, 120)\n",
        "#     #     x = x.view(-1, 1, 120, 120)  # Reshape for lower_convnet\n",
        "\n",
        "#     #     # Shared Lower ConvNet\n",
        "#     #     features = self.lower_convnet(x)\n",
        "#     #     features = features.view(-1, F, 32, 9, 9)  # Reshape back to (batch_size, F, 32, 9, 9)\n",
        "\n",
        "#     #     # Global Adjustment\n",
        "#     #     global_params = self.deform_affine(features[:, 0])  # Use the first frame for global adjustment\n",
        "#     #     global_params = global_params.view(-1, 2, 3)\n",
        "\n",
        "#     #     # Apply affine transformation to all frames\n",
        "#     #     adjusted_features = []\n",
        "#     #     for f in range(F):\n",
        "#     #         grid = nn.functional.affine_grid(global_params, x[:, f].size(), align_corners=False)\n",
        "#     #         transformed_frame = nn.functional.grid_sample(x[:, f], grid, align_corners=False)\n",
        "#     #         adjusted_features.append(transformed_frame)\n",
        "#     #     adjusted_features = torch.stack(adjusted_features, dim=1)\n",
        "\n",
        "#     #     # Local Adjustment\n",
        "#     #     shrink_features = self.deform_shrink(adjusted_features)\n",
        "\n",
        "#     #     # Bi-LSTM and Attention\n",
        "#     #     shrink_features = shrink_features.view(shrink_features.size(0), -1)\n",
        "#     #     lstm_out, _ = self.bilstm(shrink_features)\n",
        "#     #     attention_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
        "#     #     weighted_lstm_out = torch.sum(lstm_out * attention_weights, dim=1)\n",
        "\n",
        "\n",
        "#     #     return output  # Classification output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fquKAf2mZNU"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "G_NfF1Rq2fk1"
      },
      "outputs": [],
      "source": [
        "class_names = {\n",
        "    0: 'Satellite',\n",
        "    1: 'Asteroid',\n",
        "    2: 'Idk',\n",
        "    3: 'Idk2'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "u1y268rwq7ne",
        "outputId": "4902ce11-aea2-4931-d9c4-67ed61383566"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAAVCAYAAADby+34AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAABQElEQVR4nO3cUU+DMBAA4Gvn///HUh/Y1GE7ikYxu+9LCLS03FHIcuFhpbXWAgBIq56dAABwLsUAACSnGACA5BQDAJCcYgAAklMMAEByigEASE4xAADJvcwOLKVcD2pEKRFx3Ze6bl/a5WNs2Zy7HUev3Zn33n/pxO607/rrOKfa6YuymbOX0619mcipF6+Oc+nmtL2vvfsfrdtOvFoiSlu3ulyPl0/tXl+LKK8P5vSuseyM2cSrmzllmYz1ILfevG2cYS6tk8vEPY1yG+YyWLvpde6s3d3YiMv65O/29cB2dHyNfszR/jvXPnL+zFx+si5Hc1p/Cf5LNn/95szE+s2nddabs69OjQIAntb0lwH/WgwAz8mXAQBITjEAAMkpBgAgOcUAACSnGACA5BQDAJCcYgAAklMMAEByigEASO4NbvaFJ3zrhKsAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 1, 120, 120])\n",
            "Lower ConvNet: torch.Size([2, 32, 9, 9])\n",
            "Deformed affine ConvNet: torch.Size([2, 6])\n",
            "Deformed shrink ConvNet: torch.Size([2, 64])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Training Setup\n",
        "center_crop = transforms.CenterCrop(size=(120, 120)) # 250 x 170 optimal, but paper uses 120 x 120\n",
        "\n",
        "dataset = ISARDataset('test/test_labels.csv', 'test/data', transform=center_crop)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=3, shuffle=False)\n",
        "\n",
        "model = SAISARNet(num_classes=4)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "image_batch, label_batch = next(iter(train_dataloader))\n",
        "image, label = image_batch[0], int(label_batch[0])\n",
        "\n",
        "# Show image palette\n",
        "plt.imshow(dataset.palette[np.newaxis, :, :])\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False)\n",
        "\n",
        "print(image_batch.shape)\n",
        "# print(model(image_batch).shape)\n",
        "print(f'Lower ConvNet: {model.lower_convnet(image_batch).shape}')\n",
        "print(f'Deformed affine ConvNet: {model.deform_affine(image_batch).shape}')\n",
        "print(f'Deformed shrink ConvNet: {model.deform_shrink(image_batch).shape}')\n",
        "\n",
        "# # Training Loop (Conceptual)\n",
        "# for epoch in range(num_epochs):\n",
        "#     for images, labels in train_loader:\n",
        "#         # ... training logic (zero gradients, forward pass, loss calculation, backward pass, optimization step)\n",
        "\n",
        "# # Evaluation Loop (Conceptual)\n",
        "# with torch.no_grad():\n",
        "#     for images, labels in test_loader:\n",
        "#         # ... evaluation logic (model prediction, accuracy calculation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "F9ilKlwfvUyF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hi\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hi\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [8, 1, 3, 3], expected input[1, 29, 9, 9] to have 1 channels, but got 29 channels instead",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[57], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero the gradients from the previous iteration\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     33\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()         \u001b[38;5;66;03m# Backpropagation to calculate gradients\u001b[39;00m\n",
            "File \u001b[1;32md:\\Users\\wzsmith\\.conda\\envs\\torch-test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Users\\wzsmith\\.conda\\envs\\torch-test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[52], line 39\u001b[0m, in \u001b[0;36mSAISARNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhi\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Global Adjustment\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m global_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeform_affine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m global_params \u001b[38;5;241m=\u001b[39m global_params\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhi\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[1;32md:\\Users\\wzsmith\\.conda\\envs\\torch-test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Users\\wzsmith\\.conda\\envs\\torch-test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32md:\\Users\\wzsmith\\.conda\\envs\\torch-test\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32md:\\Users\\wzsmith\\.conda\\envs\\torch-test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Users\\wzsmith\\.conda\\envs\\torch-test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[49], line 22\u001b[0m, in \u001b[0;36mLowerConvNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Users\\wzsmith\\.conda\\envs\\torch-test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Users\\wzsmith\\.conda\\envs\\torch-test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32md:\\Users\\wzsmith\\.conda\\envs\\torch-test\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32md:\\Users\\wzsmith\\.conda\\envs\\torch-test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Users\\wzsmith\\.conda\\envs\\torch-test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32md:\\Users\\wzsmith\\.conda\\envs\\torch-test\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Users\\wzsmith\\.conda\\envs\\torch-test\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [8, 1, 3, 3], expected input[1, 29, 9, 9] to have 1 channels, but got 29 channels instead"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm  # Progress bar (optional, but highly recommended)\n",
        "\n",
        "num_classes = 4\n",
        "\n",
        "# Training Setup\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "model = SAISARNet(num_classes=num_classes)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust learning rate as needed\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 10  # Set the desired number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    train_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")  # Progress bar\n",
        "\n",
        "    for images, labels in progress_bar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the gradients from the previous iteration\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()         # Backpropagation to calculate gradients\n",
        "        optimizer.step()        # Update model parameters\n",
        "\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Update progress bar with loss and accuracy\n",
        "        progress_bar.set_postfix({'loss': loss.item(), 'accuracy': 100 * correct / total})\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f} Train Acc: {100 * correct / total:.2f}%\")\n",
        "\n",
        "# Save the Model (Optional)\n",
        "torch.save(model.state_dict(), \"saisar_net_model.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6BCNJPkeBEp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.ops import DeformConv2d  # Install torchvision if not already installed\n",
        "\n",
        "class SAISARNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SAISARNet, self).__init__()\n",
        "\n",
        "        # Global Image Adjustment\n",
        "        self.global_conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, kernel_size=3, padding=1),  # Input channels: 1 (grayscale)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            DeformConv2d(8, 16, kernel_size=3, padding=1),  # Deformable convolution\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        # Local Image Adjustment\n",
        "        self.local_conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            DeformConv2d(8, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        # Bi-LSTM and Attention\n",
        "        self.bilstm = nn.LSTM(input_size=16 * 14 * 14, hidden_size=128, num_layers=2, batch_first=True, bidirectional=True)\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(256, 1),  # Two directions * hidden_size\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        global_features = self.global_conv(x)  # (batch_size, 16, 14, 14)\n",
        "        local_features = self.local_conv(x)   # (batch_size, 16, 14, 14)\n",
        "\n",
        "        # Combine global and local features (choose one method)\n",
        "        # Method 1: Concatenate along channel dimension\n",
        "        # combined_features = torch.cat((global_features, local_features), dim=1)  # (batch_size, 32, 14, 14)\n",
        "\n",
        "        # Method 2: Add the features\n",
        "        combined_features = global_features + local_features\n",
        "\n",
        "        # Flatten for LSTM input\n",
        "        combined_features = combined_features.view(combined_features.size(0), -1, combined_features.size(1))\n",
        "\n",
        "        # Bi-LSTM\n",
        "        lstm_out, _ = self.bilstm(combined_features)\n",
        "\n",
        "        # Attention\n",
        "        attention_weights = self.attention(lstm_out)\n",
        "        attention_weights = torch.softmax(attention_weights, dim=1)  # Normalize attention weights\n",
        "        weighted_lstm_out = torch.sum(lstm_out * attention_weights, dim=1)\n",
        "\n",
        "        # Classification\n",
        "        output = self.classifier(weighted_lstm_out)\n",
        "        return output\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4jWLjefbmVmE"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
