{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "H2FmZxvftfj1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision.io import read_image\n",
        "from torchvision import transforms\n",
        "from torchvision.ops import DeformConv2d\n",
        "import torch.nn.functional as F\n",
        "from imageio import imread\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "SKhi6sEatrF9"
      },
      "outputs": [],
      "source": [
        "class ISARDataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "        image = imread(img_path)\n",
        "        image = transforms.ToTensor()(image).float() # Convert to float\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "6V4fHWkQl_Y7"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LowerConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv_blocks = nn.Sequential()  # Use nn.Sequential to hold blocks\n",
        "\n",
        "        in_channels = 1  # Start with 1 input channel (grayscale)\n",
        "        for out_channels, num_repeats in [(8, 4), (16, 3), (32, 3)]:\n",
        "            for _ in range(num_repeats):\n",
        "                self.conv_blocks.append(\n",
        "                    nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=0)\n",
        "                )\n",
        "                self.conv_blocks.append(nn.ReLU())\n",
        "                self.conv_blocks.append(nn.BatchNorm2d(out_channels))\n",
        "                in_channels = out_channels  # Update in_channels for next block\n",
        "\n",
        "            self.conv_blocks.append(nn.MaxPool2d(2, 2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv_blocks(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "VIrQyihkmuyo"
      },
      "outputs": [],
      "source": [
        "class DeformedAffineConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # x = x.view(-1, 1, H, W)\n",
        "        kh, kw = 3, 3\n",
        "\n",
        "        self.offset_generator1 = nn.Conv2d(32, 2 * kh * kw, kernel_size=3)  # Offset generator\n",
        "        self.deform_conv1 = DeformConv2d(32, 64, kernel_size=3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.offset_generator2 = nn.Conv2d(64, 2 * kh * kw, kernel_size=3)  # Offset generator\n",
        "        self.deform_conv2 = DeformConv2d(64, 128, kernel_size=3)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(128, 6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        offset1 = self.offset_generator1(x)\n",
        "        x = self.deform_conv1(x, offset1)\n",
        "        x = self.pool(self.bn1(F.relu(x)))\n",
        "\n",
        "        offset2 = self.offset_generator2(x)\n",
        "        x = self.deform_conv2(x, offset2)\n",
        "        x = self.bn2(F.relu(x))\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "mZrq77esc4sp"
      },
      "outputs": [],
      "source": [
        "class DeformedShrinkConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        kh, kw = 3, 3\n",
        "        self.offset_generator1 = nn.Conv2d(32, 2 * kh * kw, kernel_size=3)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.deform_conv1 = DeformConv2d(32, 64, kernel_size=3)\n",
        "\n",
        "        self.offset_generator2 = nn.Conv2d(64, 2 * kh * kw, kernel_size=3)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.deform_conv2 = DeformConv2d(64, 128, kernel_size=3)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(128, 64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        offset1 = self.offset_generator1(x)\n",
        "        x = self.deform_conv1(x, offset1)\n",
        "        x = self.pool(self.bn1(F.relu(x)))\n",
        "\n",
        "        offset2 = self.offset_generator2(x)\n",
        "        x = self.deform_conv2(x, offset2)\n",
        "        x = self.bn2(F.relu(x))\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "atwrHiFlmfSW"
      },
      "outputs": [],
      "source": [
        "class SAISARNet(nn.Module):\n",
        "    def __init__(self, num_classes, F=3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Lower ConvNet (Shared)\n",
        "        self.lower_convnet = LowerConvNet()\n",
        "\n",
        "        # Deformed Affine ConvNet (Shares parameters with Lower ConvNet)\n",
        "        self.deform_affine = nn.Sequential(\n",
        "            self.lower_convnet,\n",
        "            DeformedAffineConvNet()\n",
        "        )\n",
        "\n",
        "        # Deformed Shrink ConvNet (Independent Parameters)\n",
        "        self.deform_shrink = nn.Sequential(\n",
        "            LowerConvNet(),\n",
        "            DeformedShrinkConvNet()\n",
        "        )\n",
        "\n",
        "        # Bi-LSTM and Attention\n",
        "        self.bilstm = nn.LSTM(128*6*6, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(256, 1),  # Attention mechanism (2 * hidden_size)\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # batch_size, F, _, H, W = x.shape\n",
        "        # x = x.view(-1, 1, H, W)\n",
        "        print('hi')\n",
        "\n",
        "        # Shared Lower ConvNet\n",
        "        features = self.lower_convnet(x)\n",
        "        # features = features.view(batch_size, F, 8, 60, 60)\n",
        "        print('hi')\n",
        "\n",
        "        # Global Adjustment\n",
        "        global_params = self.deform_affine(features[:, 0])\n",
        "        global_params = global_params.view(-1, 2, 3)\n",
        "\n",
        "        print('hi')\n",
        "        # Apply affine transformation to all frames\n",
        "        adjusted_features = []\n",
        "        for f in range(F):\n",
        "            grid = F.affine_grid(global_params, x[:, f].size(), align_corners=False)\n",
        "            transformed_frame = F.grid_sample(x[:, f], grid, align_corners=False)\n",
        "            adjusted_features.append(transformed_frame)\n",
        "        adjusted_features = torch.stack(adjusted_features, dim=1)\n",
        "        print('hi')\n",
        "\n",
        "        # Local Adjustment\n",
        "        shrink_features = self.deform_shrink(features)\n",
        "\n",
        "        # Bi-LSTM and Attention\n",
        "        shrink_features = shrink_features.view(shrink_features.size(0), -1)\n",
        "        lstm_out, _ = self.bilstm(shrink_features)\n",
        "        attention_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
        "        weighted_lstm_out = torch.sum(lstm_out * attention_weights, dim=1)\n",
        "        print('hi')\n",
        "\n",
        "        # Classification\n",
        "        output = self.classifier(weighted_lstm_out)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jWLjefbmVmE"
      },
      "source": [
        "## Temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "5tr8JfVmmXza"
      },
      "outputs": [],
      "source": [
        "# pass\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# from torchvision.ops import DeformConv2d\n",
        "\n",
        "# class SAISARNet(nn.Module):\n",
        "#     # ... (your existing __init__ method remains the same)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         batch_size, F, _, H, W = x.shape  # Get dimensions (assuming input shape: (batch_size, F, 1, H, W))\n",
        "#         x = x.view(-1, 1, H, W)  # Reshape for lower_convnet (batch_size * F, 1, H, W)\n",
        "\n",
        "#         # Shared Lower ConvNet\n",
        "#         features = self.lower_convnet(x)\n",
        "#         features = features.view(batch_size, F, 32, 9, 9)  # Reshape back to (batch_size, F, 32, 9, 9)\n",
        "\n",
        "#         # Global Adjustment\n",
        "#         global_params = self.deform_affine(features[:, 0])\n",
        "#         global_params = global_params.view(-1, 2, 3)\n",
        "\n",
        "#         # Apply affine transformation to all frames\n",
        "#         adjusted_features = []\n",
        "#         for f in range(F):\n",
        "#             grid = F.affine_grid(global_params, x[:, f].size(), align_corners=False)\n",
        "#             transformed_frame = F.grid_sample(x[:, f], grid, align_corners=False)\n",
        "#             adjusted_features.append(transformed_frame)\n",
        "#         adjusted_features = torch.stack(adjusted_features, dim=1)\n",
        "\n",
        "#         # Local Adjustment\n",
        "#         shrink_features = self.deform_shrink(adjusted_features)\n",
        "#         shrink_features = shrink_features.view(batch_size, F, -1)  # Flatten for LSTM\n",
        "\n",
        "#         # Bi-LSTM and Attention\n",
        "#         lstm_out, _ = self.bilstm(shrink_features)\n",
        "\n",
        "#         attention_weights = self.attention(lstm_out)  # Calculate attention weights\n",
        "#         attention_weights = torch.softmax(attention_weights, dim=1)\n",
        "\n",
        "#         # Apply attention and aggregate features\n",
        "#         weighted_features = torch.bmm(lstm_out.transpose(1, 2), attention_weights).squeeze(2)\n",
        "\n",
        "#         # Classification\n",
        "#         output = self.classifier(weighted_features)\n",
        "#         return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "3aRVmGevdwGt"
      },
      "outputs": [],
      "source": [
        "# pass\n",
        "# class SAISARNet(nn.Module):\n",
        "#     def __init__(self, num_classes, F=3):  # F: number of frames in the sequence\n",
        "#         super().__init__()\n",
        "\n",
        "#         # self.conv_layers = nn.Sequential(\n",
        "#         #     # Block 1: Conv.8@3x3, BN/ReLU x4\n",
        "#         #     nn.Conv2d(1, 8, kernel_size=3, padding=0),  # 1 input channel (grayscale)\n",
        "#         #     nn.ReLU(),\n",
        "#         #     nn.BatchNorm2d(8),\n",
        "#         #     nn.Conv2d(8, 8, kernel_size=3, padding=0),\n",
        "#         #     nn.ReLU(),\n",
        "#         #     nn.BatchNorm2d(8),\n",
        "#         #     nn.Conv2d(8, 8, kernel_size=3, padding=0),\n",
        "#         #     nn.ReLU(),\n",
        "#         #     nn.BatchNorm2d(8),\n",
        "#         #     nn.Conv2d(8, 8, kernel_size=3, padding=0),\n",
        "#         #     nn.ReLU(),\n",
        "#         #     nn.BatchNorm2d(8),\n",
        "#         #     nn.MaxPool2d(2, 2))\n",
        "\n",
        "#         #     # Block 2: Conv.16@3x3, BN/ReLU x3\n",
        "#         # self.conv_layer2 = nn.Sequential(\n",
        "#         #     nn.Conv2d(8, 16, kernel_size=3),\n",
        "#         #     nn.ReLU(),\n",
        "#         #     nn.BatchNorm2d(16),\n",
        "#         #     nn.Conv2d(16, 16, kernel_size=3),\n",
        "#         #     nn.ReLU(),\n",
        "#         #     nn.BatchNorm2d(16),\n",
        "#         #     nn.Conv2d(16, 16, kernel_size=3),\n",
        "#         #     nn.ReLU(),\n",
        "#         #     nn.BatchNorm2d(16),\n",
        "#         #     nn.MaxPool2d(2, 2))\n",
        "#         # self.conv_layer3 = nn.Sequential(\n",
        "#         #     # # Block 3: Conv.16@3x3, BN/ReLU x3\n",
        "#         #     nn.Conv2d(16, 32, kernel_size=3),\n",
        "#         #     nn.ReLU(),\n",
        "#         #     nn.BatchNorm2d(32),\n",
        "#         #     nn.Conv2d(32, 32, kernel_size=3),\n",
        "#         #     nn.ReLU(),\n",
        "#         #     nn.BatchNorm2d(32),\n",
        "#         #     nn.Conv2d(32, 32, kernel_size=3),\n",
        "#         #     nn.ReLU(),\n",
        "#         #     nn.BatchNorm2d(32),\n",
        "#         #     nn.MaxPool2d(2, 2))  # Output 32x9x9\n",
        "\n",
        "\n",
        "#         def forward(self, x):\n",
        "#             return self.conv_layer2(self.conv_layers(x))\n",
        "\n",
        "#         # Lower ConvNet (Shared)\n",
        "#         # self.lower_convnet = nn.Sequential(*layers)\n",
        "#         #     nn.Conv2d(1, 8, kernel_size=3, padding=1),  # Input 1 channel (grayscale)\n",
        "#         #     nn.ReLU(),\n",
        "#         #     nn.BatchNorm2d(8),\n",
        "#         #     nn.MaxPool2d(2, 2),\n",
        "\n",
        "#         #     nn.Conv2d(8, 16, kernel_size=3, padding=1),\n",
        "#         #     nn.ReLU(),\n",
        "#         #     nn.BatchNorm2d(16),\n",
        "#         #     nn.MaxPool2d(2, 2),\n",
        "\n",
        "#         #     nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "#         #     nn.ReLU(),\n",
        "#         #     nn.BatchNorm2d(32),\n",
        "#         #     nn.MaxPool2d(2, 2)  # Output 32x9x9\n",
        "#         # )\n",
        "\n",
        "#         # Deformed Affine ConvNet\n",
        "#         self.deform_affine = nn.Sequential(\n",
        "#             DeformConv2d(32, 64, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(64),\n",
        "#             nn.MaxPool2d(2, 2),\n",
        "\n",
        "#             DeformConv2d(64, 128, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Flatten(),\n",
        "#             nn.Linear(128, 6)  # Output affine transformation parameters (2x3)\n",
        "#         )\n",
        "\n",
        "#         # Deformed Shrink ConvNet\n",
        "#         self.deform_shrink = nn.Sequential(\n",
        "#             DeformConv2d(32, 64, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(2, 2),\n",
        "#             DeformConv2d(64, 128, kernel_size=3, padding=1),\n",
        "#             nn.ReLU()\n",
        "#         )\n",
        "\n",
        "#         # Bi-LSTM and Attention\n",
        "#         self.bilstm = nn.LSTM(128, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
        "#         self.attention = nn.Sequential(\n",
        "#             nn.Linear(256, 1),  # Attention mechanism (2 * hidden_size)\n",
        "#             nn.Tanh()\n",
        "#         )\n",
        "#         self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "#     # def forward(self, x):\n",
        "#     #     # Assuming input x is (batch_size, F, 120, 120)\n",
        "#     #     x = x.view(-1, 1, 120, 120)  # Reshape for lower_convnet\n",
        "\n",
        "#     #     # Shared Lower ConvNet\n",
        "#     #     features = self.lower_convnet(x)\n",
        "#     #     features = features.view(-1, F, 32, 9, 9)  # Reshape back to (batch_size, F, 32, 9, 9)\n",
        "\n",
        "#     #     # Global Adjustment\n",
        "#     #     global_params = self.deform_affine(features[:, 0])  # Use the first frame for global adjustment\n",
        "#     #     global_params = global_params.view(-1, 2, 3)\n",
        "\n",
        "#     #     # Apply affine transformation to all frames\n",
        "#     #     adjusted_features = []\n",
        "#     #     for f in range(F):\n",
        "#     #         grid = nn.functional.affine_grid(global_params, x[:, f].size(), align_corners=False)\n",
        "#     #         transformed_frame = nn.functional.grid_sample(x[:, f], grid, align_corners=False)\n",
        "#     #         adjusted_features.append(transformed_frame)\n",
        "#     #     adjusted_features = torch.stack(adjusted_features, dim=1)\n",
        "\n",
        "#     #     # Local Adjustment\n",
        "#     #     shrink_features = self.deform_shrink(adjusted_features)\n",
        "\n",
        "#     #     # Bi-LSTM and Attention\n",
        "#     #     shrink_features = shrink_features.view(shrink_features.size(0), -1)\n",
        "#     #     lstm_out, _ = self.bilstm(shrink_features)\n",
        "#     #     attention_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
        "#     #     weighted_lstm_out = torch.sum(lstm_out * attention_weights, dim=1)\n",
        "\n",
        "\n",
        "#     #     return output  # Classification output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fquKAf2mZNU"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "G_NfF1Rq2fk1"
      },
      "outputs": [],
      "source": [
        "class_names = {\n",
        "    0: 'Satellite',\n",
        "    1: 'Asteroid',\n",
        "    2: 'Idk',\n",
        "    3: 'Idk2'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "u1y268rwq7ne",
        "outputId": "4902ce11-aea2-4931-d9c4-67ed61383566"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "D:\\Users\\wzsmith\\AppData\\Local\\Temp\\ipykernel_14740\\1625523307.py:13: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  image = imread(img_path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 3, 120, 120])\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [8, 1, 3, 3], expected input[4, 3, 120, 120] to have 1 channels, but got 3 channels instead",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[49], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(image_batch\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# print(model.forward(image_batch).shape)\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLower ConvNet: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower_convnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeformed affine ConvNet: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mdeform_affine(image_batch)\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeformed shrink ConvNet: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mdeform_shrink(image_batch)\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[1;32md:\\Users\\wzsmith\\.conda\\envs\\torch2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Users\\wzsmith\\.conda\\envs\\torch2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[38], line 22\u001b[0m, in \u001b[0;36mLowerConvNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Users\\wzsmith\\.conda\\envs\\torch2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Users\\wzsmith\\.conda\\envs\\torch2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32md:\\Users\\wzsmith\\.conda\\envs\\torch2\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32md:\\Users\\wzsmith\\.conda\\envs\\torch2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Users\\wzsmith\\.conda\\envs\\torch2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32md:\\Users\\wzsmith\\.conda\\envs\\torch2\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\Users\\wzsmith\\.conda\\envs\\torch2\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [8, 1, 3, 3], expected input[4, 3, 120, 120] to have 1 channels, but got 3 channels instead"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Training Setup\n",
        "center_crop = transforms.CenterCrop(size=(120, 120)) # 250 x 170 optimal, but paper uses 120 x 120\n",
        "\n",
        "dataset = ISARDataset('test/test_labels.csv', 'data/', transform=center_crop)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "model = SAISARNet(num_classes=4)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "image_batch, label_batch = next(iter(train_dataloader))\n",
        "image, label = image_batch[0], int(label_batch[0])\n",
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False)\n",
        "\n",
        "# After you've created your model (e.g., model = SAISARNet(...))\n",
        "\n",
        "# for name, param in model.named_parameters():\n",
        "#     print(f'Parameter name: {name}, Shape: {param.size()}, Values: {param}')\n",
        "\n",
        "print(image_batch.shape)\n",
        "# print(model.forward(image_batch).shape)\n",
        "print(f'Lower ConvNet: {model.lower_convnet(image_batch).shape}')\n",
        "print(f'Deformed affine ConvNet: {model.deform_affine(image_batch).shape}')\n",
        "print(f'Deformed shrink ConvNet: {model.deform_shrink(image_batch).shape}')\n",
        "\n",
        "# # Training Loop (Conceptual)\n",
        "# for epoch in range(num_epochs):\n",
        "#     for images, labels in train_loader:\n",
        "#         # ... training logic (zero gradients, forward pass, loss calculation, backward pass, optimization step)\n",
        "\n",
        "# # Evaluation Loop (Conceptual)\n",
        "# with torch.no_grad():\n",
        "#     for images, labels in test_loader:\n",
        "#         # ... evaluation logic (model prediction, accuracy calculation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9ilKlwfvUyF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm  # Progress bar (optional, but highly recommended)\n",
        "\n",
        "\n",
        "# Training Setup\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "model = SAISARNet(num_classes=num_classes)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust learning rate as needed\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 10  # Set the desired number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    train_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")  # Progress bar\n",
        "\n",
        "    for images, labels in progress_Fbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the gradients from the previous iteration\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()         # Backpropagation to calculate gradients\n",
        "        optimizer.step()        # Update model parameters\n",
        "\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Update progress bar with loss and accuracy\n",
        "        progress_bar.set_postfix({'loss': loss.item(), 'accuracy': 100 * correct / total})\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f} Train Acc: {100 * correct / total:.2f}%\")\n",
        "\n",
        "# Save the Model (Optional)\n",
        "torch.save(model.state_dict(), \"saisar_net_model.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6BCNJPkeBEp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.ops import DeformConv2d  # Install torchvision if not already installed\n",
        "\n",
        "class SAISARNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SAISARNet, self).__init__()\n",
        "\n",
        "        # Global Image Adjustment\n",
        "        self.global_conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, kernel_size=3, padding=1),  # Input channels: 1 (grayscale)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            DeformConv2d(8, 16, kernel_size=3, padding=1),  # Deformable convolution\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        # Local Image Adjustment\n",
        "        self.local_conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            DeformConv2d(8, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        # Bi-LSTM and Attention\n",
        "        self.bilstm = nn.LSTM(input_size=16 * 14 * 14, hidden_size=128, num_layers=2, batch_first=True, bidirectional=True)\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(256, 1),  # Two directions * hidden_size\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        global_features = self.global_conv(x)  # (batch_size, 16, 14, 14)\n",
        "        local_features = self.local_conv(x)   # (batch_size, 16, 14, 14)\n",
        "\n",
        "        # Combine global and local features (choose one method)\n",
        "        # Method 1: Concatenate along channel dimension\n",
        "        # combined_features = torch.cat((global_features, local_features), dim=1)  # (batch_size, 32, 14, 14)\n",
        "\n",
        "        # Method 2: Add the features\n",
        "        combined_features = global_features + local_features\n",
        "\n",
        "        # Flatten for LSTM input\n",
        "        combined_features = combined_features.view(combined_features.size(0), -1, combined_features.size(1))\n",
        "\n",
        "        # Bi-LSTM\n",
        "        lstm_out, _ = self.bilstm(combined_features)\n",
        "\n",
        "        # Attention\n",
        "        attention_weights = self.attention(lstm_out)\n",
        "        attention_weights = torch.softmax(attention_weights, dim=1)  # Normalize attention weights\n",
        "        weighted_lstm_out = torch.sum(lstm_out * attention_weights, dim=1)\n",
        "\n",
        "        # Classification\n",
        "        output = self.classifier(weighted_lstm_out)\n",
        "        return output\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4jWLjefbmVmE"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
